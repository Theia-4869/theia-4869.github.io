<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="theme" content="hugo-academic">
        <meta name="generator" content="Hugo 0.19">
        <meta name="author" content="Qizhe Zhang">
        <meta name="description" content="Theia's Website">
        <link rel="stylesheet" href="./css/highlight.min.css">
        <link rel="stylesheet" href="./css/bootstrap.min.css">
        <link rel="stylesheet" href="./css/font-awesome.min.css">
        <link rel="stylesheet" href="./css/academicons.min.css">
        <link rel="stylesheet" href="//fonts.googleapis.com./css?family=Lato:400,700|Merriweather|Roboto+Mono">
        <link rel="stylesheet" href="./css/hugo-academic.css">
        <link rel="alternate" href="https://theia-4869.github.io/index.html" type="application/rss+xml" title="Qizhe Zhang's Homepage">
        <link rel="feed" href="https://theia-4869.github.io/index.html" type="application/rss+xml" title="Qizhe Zhang's Homepage">
        <link rel="icon" type="image/png" href="./img/icon.png">
        <link rel="shortcut icon" type="image/x-icon" href="./img/favicon.ico?">
        <link rel="apple-touch-icon" type="image/png" href="./img/icon.png">
        <link rel="canonical" href="https://theia-4869.github.io/">
        <title>Theia's Website</title>
        <script>var _hmt = _hmt || []; (function() {
                var hm = document.createElement("script");
                hm.src = "https://hm.baidu.com/hm.js?c83bf8b7e066e7c41af36a85a5651bf2";
                var s = document.getElementsByTagName("script")[0];
                s.parentNode.insertBefore(hm, s);
            })();</script>
    </head>

    <body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">
        <nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
            <div class="container">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="/">Qizhe Zhang's Homepage</a></div>
                <div class="collapse navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li class="nav-item">
                            <a href="/#AboutMe" data-target="#AboutMe">
                                <span>About Me</span></a>
                        </li>
						<li class="nav-item">
                            <a href="/#Experience" data-target="#Experience">
                                <span>Experience</span></a>
						</li>
                        <li class="nav-item">
                            <a href="/#Publications" data-target="#Publications">
                                <span>Publications</span></a>
                        </li>
						<li class="nav-item">
                            <a href="/#Plan" data-target="#Plan">
                                <span>Plan</span></a>
                        <li class="nav-item">
                            <a href="/#Contact" data-target="#Contact">
                                <span>Contact</span></a>
                        </li>
                    </ul>
                </div>
            </div>
        </nav>
		
        <span id="homepage" style="display: none"></span>
        <section id="AboutMe" class="home-section">
            <div class="container">
                <div class="row" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
                    <div class="col-xs-12 col-md-4">
                        <div id="profile">
                            <div class="portrait" itemprop="image" style="background-image: url('./img/photo(white).jpeg');"></div>
                            <div class="portrait-title">
                                <h2 itemprop="name">Qizhe Zhang</h2>
                                <h3 itemprop="institution"><a href="https://english.pku.edu.cn/">PKU</a> Ph.D. candidate</h3>
                                <h4 itemprop="language"><a href="https://theia-4869.github.io/cn/">简体中文</a></h4>
                            </div>
                            <p>
                                <a href="https://scholar.google.com/citations?hl=en&user=cdAi_uIAAAAJ"><i class="ai ai-google-scholar ai-2x"></i></a> &nbsp  
								<a href="https://github.com/Theia-4869"><i class="fa fa-github fa-2x"></i></a> &nbsp  
                                <a href="./cv/resume.pdf"><i class="ai ai-cv ai-2x"></i></a> &nbsp  
                            </p>
                        </div>
                    </div>
                    <div class="col-xs-12 col-md-8" itemprop="description">
                        <h1 id="biography">About Me</h1>
                        <p> I am currently a Ph.D. candidate at <a href="https://pku-hmi-lab.github.io/HMI-Web/">HMI Lab</a>, <a href="https://idm.pku.edu.cn/en/">NERCV²T</a>, <a href="https://cs.pku.edu.cn/English/Home.htm">School of Computer Science</a>, 
							<a href="https://english.pku.edu.cn/">Peking University</a>, supervised by Prof. <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>.
							Before that, I received my Bachelor's degree in Artificial Intelligence (Turing Honor Degree) from Peking University, where I also obtained a Bachelor's degree in Economics.</p>
						<h2>Research Interests</h2>
						<p> My research interests lie in computer vision and multimodal learning, including visual foundation models, vision language models, visual complex reasoning, 
							visual token compression, visual continual learning, and embodied artificial intelligence. The overall goal of my research is to develop a large-scale efficient visual perception system 
							with human-like expression, adaptation, and generalization, equipped with powerful abilities including fundamental perception, cognitive reasoning, and autonomous creativity.</p>
                        <p> More specifically, my research interests include:</p>
                            <ol>
								<li> Visual Foundation Models (VFMs) </li>
								<li> Vision Language Models (VLMs / MLLMs / dVLMs) </li>
								<li> Visual Complex Reasoning (Visual CoT / Thinking with images) </li>
								<li> Visual Token Compression (Token Pruning / KV Cache Compression) </li>
								<li> Visual Continual Learning (Memory Mechanism / Slow-Fast System) </li>
								<li> Embodied Artificial Intelligence (Robotics / Autonomous Driving) </li>
                            </ol>               	
					</div>

                    <div class="col-md-12">
                        <div class="col-sm-4">
                            <h3>Education</h3>
                            <ul class="ul-edu fa-ul">
                                <li>
                                    <i class="fa-li fa fa-graduation-cap"></i>
                                    <div class="description">
                                        <p class="course">Ph.D. Candidate in Visual Information Processing and Brain-inspired Intelligence
                                            <br>Sep. 2023 -- Jun. 2028 (ETA)</p>
                                        <p class="institution">Peking University, Beijing, China</p></div>
                                </li>
                                <li>
                                    <i class="fa-li fa fa-graduation-cap"></i>
                                    <div class="description">
                                        <p class="course">Bachelor of Intelligence Science and Technology & Economics (Dual Degree)
                                            <br>Sep. 2019 -- Jun. 2023</p>
                                        <p class="institution">Peking University, Beijing, China</p></div>
                                </li>
                            </ul>
                        </div>     

                        
						<div class="col-sm-8">
                            <h3>News</h3>
								<ul class="ul-interests">
									<li> 06/2025: One paper is accepted by <b>ICCV 2025</b>. (<a href="/#VisPruner">VisPruner</a>) </li>
									<li> 05/2025: One paper is accepted by <b>ICML 2025</b>. (SAN) </li>
									<li> 02/2025: One paper is accepted by <b>CVPR 2025</b>. (MoVE-KD) </li>
									<li> 02/2024: Two papers are accepted by <b>CVPR 2024</b>. (GPS, ADMA) </li>
									<li> 01/2024: One paper is accepted by <b>ICRA 2024</b>. (BiCross) </li>
									<li> 12/2023: One paper is accepted by <b>AAAI 2024</b>. (SVDP) </li>
								</ul>
						</div>

                    </div>
                </div>
            </div>
        </section>
		
		<section id="Experience" class="home-section">
			<div class="container">
				<div class="row">
					<div class="col-xs-12 col-md-4 section-heading">
						<h1>Experience</h1></div>
					<div class="col-xs-12">
						<hr style="filter:alpha(opacity=100,finishopacity=0,style=2);background-color:#67c077;height:8px">
							<ul class="ul-edu fa-ul">
								<li>
								    <i class="fa-li fa fa-briefcase"></i>
								    <div class="description">
								        <p class="course">Intern at AI Lab (Model Efficiency for MLLM)
								            <br>Mar. 2024 -- Now</p>
								        <p class="institution"><a href="https://www.bytedance.com/en/">ByteDance</a>, Beijing, China</p>
								    </div>
								</li>
								<li>
								    <i class="fa-li fa fa-briefcase"></i>
								    <div class="description">
								        <p class="course">Intern in AGI (Memory Mechanism for MLLM)
								            <br>Jul. 2023 -- Sep. 2023</p>
								        <p class="institution"><a href="https://www.baai.ac.cn/english.html">BAAI</a>, Beijing, China</p>
								    </div>
								</li>
                                <li>
                                    <i class="fa-li fa fa-briefcase"></i>
                                    <div class="description">
                                        <p class="course">Intern in Computer Vision (Autonomous Driving)
                                            <br>Sep. 2022 -- Feb. 2023</p>
                                        <p class="institution"><a href="https://www.oppo.com/en/">OPPO</a>, Beijing, China</p>
                                    </div>
                                </li>
                                <li>
                                    <i class="fa-li fa fa-briefcase"></i>
                                    <div class="description">
                                        <p class="course">Intern at GCV Lab (Multi-Modal Learning)
                                            <br>Oct. 2021 -- Feb. 2022</p>
                                        <p class="institution"><a href="https://bigai.ai/">BIGAI</a>, Beijing, China</p>
                                    </div>
                                </li>
							</ul>
						</div>
					</div>
				</div>
		</section>
		
        <section id="Publications" class="home-section">
            <div class="container">
                <div class="row">
                    <div class="col-xs-12 col-md-6 section-heading">
                        <h1>Publications</h1>
					</div>
                    <div class="col-xs-12">
                        <hr style="filter:alpha(opacity=100,finishopacity=0,style=2);background-color:#67c077;height:8px">
						
						<div id="CDPruner" class="row">
							<div class="col-md-3">
								<img class="img-fluid img-rounded" src="./img/papers/CDPruner.png" style="border:1px solid black" alt="">
							</div>
							<div class="col-md-9">
								<b><font size="6" color="black">Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs</font></b><br>
								<a href="https://scholar.google.com/citations?user=cdAi_uIAAAAJ" target="_blank"><b>Qizhe Zhang</b></a>,
								<a href="https://scholar.google.com/citations?user=AUVUNusAAAAJ" target="_blank">Mengzhen Liu</a>,
								<a>Lichen Li</a>,
								<a href="https://scholar.google.com/citations?user=3vArSU0AAAAJ" target="_blank">Ming Lu</a>,
								<a href="https://scholar.google.com/citations?user=dXj1WskAAAAJ" target="_blank">Yuan Zhang</a>,
								<a href="https://scholar.google.com/citations?user=YvgX3sUAAAAJ" target="_blank">Junwen Pan</a>,
								<a href="https://scholar.google.com/citations?user=iHoGTt4AAAAJ" target="_blank">Qi She</a>,
								<a href="https://scholar.google.com/citations?user=voqw10cAAAAJ" target="_blank">Shanghang Zhang†</a>
								<br>
								<b>Arxiv 2025
								<a href="https://arxiv.org/abs/2506.10967" target="_blank"> <small>[Paper]</small></a>
								<a href="https://github.com/Theia-4869/CDPruner" target="_blank"> <small>[Code]</small></a>
								<a href="https://theia-4869.github.io/CDPruner" target="_blank"> <small>[Website]</small></a>
								<br></b>
								<i>" We propose CDPruner as A training-free and model-agnostic visual token pruning method for 
									MLLM inference acceleration by maximizing the conditional diversity of retained tokens. "</i>
							</div>
						</div><hr>

						<div id="VisPruner" class="row">
							<div class="col-md-3">
								<img class="img-fluid img-rounded" src="./img/papers/VisPruner.png" style="border:1px solid black" alt="">
							</div>
							<div class="col-md-9">
								<b><font size="6" color="black">Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs</font></b><br>
								<a href="https://scholar.google.com/citations?user=cdAi_uIAAAAJ" target="_blank"><b>Qizhe Zhang</b></a>,
								<a href="https://scholar.google.com/citations?user=lQaJlDYAAAAJ" target="_blank">Aosong Cheng</a>,
								<a href="https://scholar.google.com/citations?user=3vArSU0AAAAJ" target="_blank">Ming Lu</a>,
								<a href="https://scholar.google.com/citations?user=YlL3xN4AAAAJ" target="_blank">Renrui Zhang</a>,
								<a href="https://scholar.google.com/citations?user=hfWWE7MAAAAJ" target="_blank">Zhiyong Zhuo</a>,
								<a href="https://scholar.google.com/citations?user=femNsd0AAAAJ" target="_blank">Jiajun Cao</a>,
								<a href="https://scholar.google.com/citations?user=RdDDC9MAAAAJ" target="_blank">Shaobo Guo</a>,
								<a href="https://scholar.google.com/citations?user=iHoGTt4AAAAJ" target="_blank">Qi She</a>,
								<a href="https://scholar.google.com/citations?user=voqw10cAAAAJ" target="_blank">Shanghang Zhang†</a>
								<br>
								<b>ICCV 2025
								<a href="https://arxiv.org/abs/2412.01818" target="_blank"> <small>[Paper]</small></a>
								<a href="https://github.com/Theia-4869/VisPruner" target="_blank"> <small>[Code]</small></a>
								<a href="https://theia-4869.github.io/VisPruner" target="_blank"> <small>[Website]</small></a>
								<br></b>
								<i>" We propose VisPruner as a plug-and-play method that utilizes visual cues for 
									more effective token pruning in large vision language models. "</i>
							</div>
						</div><hr>
						
						<div id="GPS" class="row">
							<div class="col-md-3">
								<img class="img-fluid img-rounded" src="./img/papers/GPS.png" style="border:1px solid black" alt="">
							</div>
							<div class="col-md-9">
								<b><font size="6" color="black">Gradient-based Parameter Selection for Efficient Fine-Tuning</font></b><br>
								<a href="https://scholar.google.com/citations?user=Mg-Y15YAAAAJ" target="_blank">Zhi Zhang*</a>,
								<a href="https://scholar.google.com/citations?user=cdAi_uIAAAAJ" target="_blank"><b>Qizhe Zhang*</b></a>,
								<a href="https://scholar.google.com/citations?user=1uyjK9IAAAAJ" target="_blank">Zijun Gao</a>,
								<a href="https://scholar.google.com/citations?user=YlL3xN4AAAAJ" target="_blank">Renrui Zhang</a>,
								<a href="https://scholar.google.com/citations?user=jqOFBGoAAAAJ" target="_blank">Ekaterina Shutova</a>,
								<a href="https://scholar.google.com/citations?user=Do5jf8oAAAAJ" target="_blank">Shiji Zhou</a>,
								<a href="https://scholar.google.com/citations?user=voqw10cAAAAJ" target="_blank">Shanghang Zhang†</a>
								<br>
								<b>CVPR 2024
								<a href="https://arxiv.org/abs/2312.10136" target="_blank"> <small>[Paper]</small></a>
								<a href="https://github.com/FightingFighting/GPS" target="_blank"> <small>[Code]</small></a>
								<br></b>
								<i>" We propose a novel gradient-based parameter selection (GPS) method for effeicient fine-tuning. 
									GPS does not introduce any additional storage or computational cost during both training and inference stages. 
									Moreover, it possesses model-agnostic and task-adaptive properties, achieving outstanding performance. "</i>
							</div>
						</div><hr>
            
						<div id="BiCross" class="row">
							<div class="col-md-3">
								<img class="img-fluid img-rounded" src="./img/papers/BiCross.png" style="border:1px solid black" alt="">
							</div>
							<div class="col-md-9">
								<b><font size="6" color="black">Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer</font></b><br>
								<a href="https://scholar.google.com/citations?user=cPki5sUAAAAJ" target="_blank">Jiaming Liu*</a>,
								<a href="https://scholar.google.com/citations?user=cdAi_uIAAAAJ" target="_blank"><b>Qizhe Zhang*</b></a>,
								<a href="https://scholar.google.com/citations?user=xrYnfwcAAAAJ" target="_blank">Jianing Li</a>,
								<a href="https://scholar.google.com/citations?user=3vArSU0AAAAJ" target="_blank">Ming Lu</a>,
								<a href="https://scholar.google.com/citations?user=knvEK4AAAAAJ" target="_blank">Tiejun Huang</a>,
								<a href="https://scholar.google.com/citations?user=voqw10cAAAAJ" target="_blank">Shanghang Zhang†</a>
								<br>
								<b>ICRA 2024
								<a href="https://arxiv.org/abs/2208.12527" target="_blank"> <small>[Paper]</small></a>
								<a href="https://github.com/Theia-4869/BiCross" target="_blank"> <small>[Code]</small></a>
								<br></b>
								<i>" We propose a novel cross-modality cross-domain (BiCross) framework for unsupervised spike depth estimation. 
									To be mentioned, we are the first to exploit the opensource RGB datasets to help unsupervised learning for spike depth estimation. "</i>
							</div>
						</div><hr>
						
						<div id="SAN" class="row">
							<div class="col-md-3">
								<img class="img-fluid img-rounded" src="./img/papers/SAN.png" style="border:1px solid black" alt="">
							</div>
							<div class="col-md-9">
								<b><font size="6" color="black">SAN: Hypothesizing Long-Term Synaptic Development and Neural Engram Mechanism in Scalable Model's Parameter-Efficient Fine-Tuning</font></b><br>
								<a href="https://scholar.google.com/citations?user=2Of6xZUAAAAJ" target="_blank">Gaole Dai</a>,
								<a href="https://scholar.google.com/citations?user=0WYAYQ8AAAAJ" target="_blank">Yiming Tang</a>,
								<a href="https://scholar.google.com/citations?user=TxeAbWkAAAAJ" target="_blank">Chunkai Fan</a>,
								<a href="https://scholar.google.com/citations?user=cdAi_uIAAAAJ" target="_blank"><b>Qizhe Zhang</b></a>,
								<a href="https://scholar.google.com/citations?user=Mg-Y15YAAAAJ" target="_blank">Zhi Zhang</a>,
								<a href="https://scholar.google.com/citations?user=hQ-J_eAAAAAJ" target="_blank">Yulu Gan</a>,
								<a>Chengching Tseng</a>,
								<a href="https://scholar.google.com/citations?user=voqw10cAAAAJ" target="_blank">Shanghang Zhang†</a>
								<a href="https://scholar.google.com/citations?user=knvEK4AAAAAJ" target="_blank">Tiejun Huang</a>,
								<br>
								<b>ICML 2025
								<a href="https://arxiv.org/abs/2409.06706" target="_blank"> <small>[Paper]</small></a>
								<a href="https://github.com/daviddaiiiii/san-peft" target="_blank"> <small>[Code]</small></a>
								<br></b>
								<i>" We propose Synapse and Neuron (SAN), which decomposes and propagates scaling components from anterior feature 
									adjusting vectors towards posterior weight matrices. SAN is theoretically grounded in Long-Term Potentiation/Depression 
									phenomena, which govern synapse development through neurotransmitter release modulation. "</i>
							</div>
						</div><hr>
						
						<div id="MoVE-KD" class="row">
							<div class="col-md-3">
								<img class="img-fluid img-rounded" src="./img/papers/MoVE-KD.png" style="border:1px solid black" alt="">
							</div>
							<div class="col-md-9">
								<b><font size="6" color="black">MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders</font></b><br>
								<a href="https://scholar.google.com/citations?user=femNsd0AAAAJ" target="_blank">Jiajun Cao</a>,
								<a href="https://scholar.google.com/citations?user=dXj1WskAAAAJ" target="_blank">Yuan Zhang</a>,
								<a href="https://scholar.google.com/citations?user=jkcRdBgAAAAJ" target="_blank">Tao Huang</a>,
								<a href="https://scholar.google.com/citations?user=3vArSU0AAAAJ" target="_blank">Ming Lu</a>,
								<a href="https://scholar.google.com/citations?user=cdAi_uIAAAAJ" target="_blank"><b>Qizhe Zhang</b></a>,
								<a href="https://scholar.google.com/citations?user=R5iSLPQAAAAJ" target="_blank">Ruichuan An</a>,
								<a href="https://scholar.google.com/citations?user=R5iSLPQAAAAJ" target="_blank">Ningning Ma</a>,
								<a href="https://scholar.google.com/citations?user=vOAzYlcAAAAJ" target="_blank">Shanghang Zhang†</a>
								<br>
								<b>CVPR 2025
								<a href="https://arxiv.org/abs/2501.01709" target="_blank"> <small>[Paper]</small></a>
								<a href="https://github.com/hey-cjj/MoVE-KD" target="_blank"> <small>[Code]</small></a>
								<br></b>
								<i>" We propose Mixture-of-Visual-Encoder Knowledge Distillation (MoVE-KD), a novel framework that 
									distills the unique proficiencies of multiple vision encoders into a single, efficient encoder model. "</i>
							</div>
						</div><hr>
						
						<div id="ADMA" class="row">
							<div class="col-md-3">
								<img class="img-fluid img-rounded" src="./img/papers/ADMA.png" style="border:1px solid black" alt="">
							</div>
							<div class="col-md-9">
								<b><font size="6" color="black">Adaptive Distribution Masked Autoencoders for Continual Test-Time Adaptation</font></b><br>
								<a href="https://scholar.google.com/citations?user=cPki5sUAAAAJ" target="_blank">Jiaming Liu*</a>,
								<a href="https://scholar.google.com/citations?user=4IZPfRcAAAAJ" target="_blank">Ran Xu*</a>,
								<a href="https://scholar.google.com/citations?user=NcJc-RwAAAAJ" target="_blank">Senqiao Yang*</a>,
								<a href="https://scholar.google.com/citations?user=YlL3xN4AAAAJ" target="_blank">Renrui Zhang†</a>,
								<a href="https://scholar.google.com/citations?user=cdAi_uIAAAAJ" target="_blank"><b>Qizhe Zhang</b></a>,
								<a href="https://scholar.google.com/citations?user=NfSsLncAAAAJ" target="_blank">Zehui Chen</a>,
								<a href="https://scholar.google.com/citations?user=fWDoWsQAAAAJ" target="_blank">Yandong Guo</a>,
								<a href="https://scholar.google.com/citations?user=voqw10cAAAAJ" target="_blank">Shanghang Zhang‡</a>
								<br>
								<b>CVPR 2024
								<a href="https://arxiv.org/abs/2312.12480" target="_blank"> <small>[Paper]</small></a>
								<a href="https://github.com/RanXu2000/continual-mae" target="_blank"> <small>[Code]</small></a>
								<a href="https://sites.google.com/view/continual-mae" target="_blank"> <small>[Website]</small></a>
								<br></b>
								<i>" We propose Adaptive Distribution Masked Autoencoders (ADMA) as a novel continual self-supervised method. 
									ADMA enhances the extraction of target domain knowledge while mitigating the accumulation of distribution shifts. "</i>
							</div>
						</div><hr>
						
						<div id="SVDP" class="row">
							<div class="col-md-3">
								<img class="img-fluid img-rounded" src="./img/papers/SVDP.png" style="border:1px solid black" alt="">
							</div>
							<div class="col-md-9">
								<b><font size="6" color="black">Exploring Sparse Visual Prompt for Cross-domain Semantic Segmentation</font></b><br>
								<a href="https://scholar.google.com/citations?user=NcJc-RwAAAAJ" target="_blank">Senqiao Yang*</a>,
								<a href="https://scholar.google.com/citations?user=0Nf9tWoAAAAJ" target="_blank">Jiarui Wu*</a>,
								<a href="https://scholar.google.com/citations?user=cPki5sUAAAAJ" target="_blank">Jiaming Liu*</a>,
								<a href="https://scholar.google.com/citations?user=vkQ5_LIAAAAJ" target="_blank">Xiaoqi Li</a>,
								<a href="https://scholar.google.com/citations?user=cdAi_uIAAAAJ" target="_blank"><b>Qizhe Zhang</b></a>,
								<a href="https://scholar.google.com/citations?user=QdUeY3IAAAAJ" target="_blank">Mingjie Pan</a>,
								<a href="https://scholar.google.com/citations?user=voqw10cAAAAJ" target="_blank">Shanghang Zhang†</a>
								<br>
								<b>AAAI 2024
								<a href="https://arxiv.org/abs/2303.09792" target="_blank"> <small>[Paper]</small></a>
								<a href="https://github.com/Anonymous-012/SVDP" target="_blank"> <small>[Code]</small></a>
								<a href="https://senqiaoyang.com/project/svdp" target="_blank"> <small>[Website]</small></a>
								<br></b>
								<i>" We propose a novel Sparse Visual Domain Prompts (SVDP) approach for dense prediction TTA tasks, 
									which holds minimal trainable parameters in the image-level prompt and reserves more spatial information of the input. "</i>
							</div>
						</div><hr>
			
                    </div>
                </div>
            </div>
        </section>

    <section id="Plan" class="home-section">
        <div class="container">
            <div class="row">
                <div class="col-xs-12 col-md-4 section-heading">
                    <h1>Plan</h1>
				</div>
                <div class="col-xs-12">
                    <hr style="filter:alpha(opacity=100,finishopacity=0,style=2);background-color:#67c077;height:8px">
                        <ul class="ul-interests">
							<li> Visual Token Pruning Evaluation Framework </li>
                            <li> KV Cache Compression for LVLM/dVLM </li>
							<li> Visual CoT Length Reduction </li>
							<li> Thinking with Auxiliary Lines for Plane Geometry </li>
						</ul>
                    </div>
                </div>
            </div>
        </div>

        <section id="Contact" class="home-section">
            <div class="container">
                <div class="row">
                    <div class="col-xs-12 col-md-4 section-heading">
                        <h1>Contact</h1></div>
                    <div class="col-xs-12 col-md-8">
                        <ul class="fa-ul">
                            <li>
                                <i class="fa-li fa fa-envelope fa-2x" aria-hidden="true"></i>
                                <span>theia@pku.edu.cn &nbsp theia4869@gmail.com</span>
                            </li>
							<li>
                                <i class="fa-li fa fa-phone fa-2x" aria-hidden="true"></i>
                                <span>+86 &middot 18810920885 &nbsp +86 &middot 18700432951</span>
							</li>
							<li>
                                <i class="fa-li fa fa-wechat fa-2x" aria-hidden="true"></i>
                                <span>Theia-4869</span>
							</li>
                        </ul>
						<iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d12228.074026414926!2d116.29980640187854!3d39.98569175826669!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x35f0514b0d02a275%3A0x7bc19cd881ca04cb!2sPeking%20University!5e0!3m2!1sen!2s!4v1662225479852!5m2!1sen!2s" width="600" height="400" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
                    </div>
                </div>
            </div>
        </section>

        <br>
        <footer class="site-footer">
            <div class="container">
                <p class="powered-by">© 2022-2024 Qizhe Zhang &middot; Powered by the
                    <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic theme</a> for
                    <a href="http://gohugo.io" target="_blank">Hugo</a>.
                    <span class="pull-right" aria-hidden="true">
                        <a href="#" id="back_to_top">
                            <span class="button_icon">
                                <i class="fa fa-chevron-up fa-2x"></i></span>
                        </a>
                    </span>
                </p>
            </div>
        </footer>
        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
        <script src="./js/jquery-1.12.3.min.js"></script>
        <script src="./js/bootstrap.min.js"></script>
        <script src="./js/isotope.pkgd.min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.1/imagesloaded.pkgd.min.js"></script>
        <script src="./js/hugo-academic.js"></script-->
		<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML-full"></script>
    </body>
</html>
